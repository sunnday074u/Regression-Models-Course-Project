---
title: "Automatic or Manual Transmission: Which Has Better MPG?"
author: "Sunday Edisi"
date: "May 30, 2017"
output: pdf_document
---

## Executive Summary

This paper explores the relationship between miles-per-gallon (MPG) and other variables in the mtcars data set. In particular, the analysis attempts to determine whether an automatic or manual transmission is better for MPG, and quantifies the MPG difference.
The Analysis section of this document focuses on inference with a simple linear regression model and a multiple regression model. Both models support the conclusion that the cars in this study with manual transmissions have on average significantly higher MPG's than cars with automatic transmissions.
This conclusion holds whether we consider the relationship between MPG and transmission type alone or transmission type together with 2 other predictors: wt / weight; and qsec / 1/4 mile time.
In the simple model, the mean MPG difference is 7.245 MPG; the average MPG for cars with automatic transmissions is 17.147 MPG, and the average MPG for cars with manual transmissions is 24.392 MPG. In the multiple regression model, the MPG difference is 2.9358 MPG at the mean weight and qsec.
Exploratory analysis and visualizations are located in the Appendix to this document.
Analysis


Simple Linear Regression - lm(mpg ~ am, data = mtcars)
```{r}
data(mtcars)
n <- length(mtcars$mpg)
alpha <- 0.05
fit <- lm(mpg ~ am, data = mtcars)
coef(summary(fit))
```
The beta0 / intercept coefficient is mean MPG for cars with automatic transmissions; the beta1 / am coefficient is the mean increase in MPG for cars with manual transmissions (am = 1). The sum beta0 + beta1 is our mean MPG for cars with manual transmissions.
Using the output above, we can calculate a 95% confidence interval for beta1 (mean MPG difference) as follows:
  

```{r}  
pe <- coef(summary(fit))["am", "Estimate"]
se <- coef(summary(fit))["am", "Std. Error"]
tstat <- qt(1 - alpha/2, n - 2)  # n - 2 for model with intercept and slope
pe + c(-1, 1) * (se * tstat)
```

The p-value of 2.850207410^{-4} for beta1 is small and the CI does not include zero, so we can reject null in favor of the alternative hypothesis that there is a significant difference in MPG between the two groups at alpha = 0.05.

Multiple Regression - lm(mpg ~ wt + qsec + am, data=mtcars)
The predictors wt (weight), qsec (1/4 mile time) and am (transmission type) were first selected in an automated fashion using the bestglm package. This set of predictors yields the highest adjusted R-squared. This result agrees with what you arrive at by following this logic:
1.	Start with the predictor whose correlation with mpg is highest (wt);
2.	Eliminate from the model variables that are highly correlated with wt;
3.	Add the remaining predictor, qsec, which is nearly orthogonal to wt; and
4.	Add our variable of interest, am, to see if it is a significant predictor.


$BestModel

```{r} 
# Call:
#lm(formula = y ~ ., data = data.frame(Xy[, c(bestset[-1], FALSE), 
#     drop = FALSE], y = y))
```

   
# fit a model using the regressors suggested by bestglm residual plot is in
# Appendix

```{r}
bestfit <- lm(mpg ~ wt + qsec + am, data = mtcars)
coef(summary(bestfit))
```

Using the output above, we can calculate a 95% confidence interval for beta3 / am as follows:

```{r}
pe <- coef(summary(bestfit))["am", "Estimate"]
se <- coef(summary(bestfit))["am", "Std. Error"]
tstat <- qt(1 - alpha/2, n - 2)  # n - 2 for model with intercept and slope
pe + c(-1, 1) * (se * tstat)
```

The p-value of 0.0467155 for beta3 is small and the CI does not include zero, so we can reject null in favor of the alternative hypothesis that there is a significant difference in MPG between the two groups at alpha = 0.05.
Nested Model Testing
# nested model testing of the model selected by bestglm

```{r}
fit1 <- lm(mpg ~ wt, data = mtcars)
fit2 <- update(fit1, mpg ~ wt + qsec)
fit3 <- update(fit2, mpg ~ wt + qsec + am)
anova(fit1, fit2, fit3)
```

# Analysis of Variance Table
 
# Model 1: mpg ~ wt
# Model 2: mpg ~ wt + qsec
# Model 3: mpg ~ wt + qsec + am

The nested model test demonstrated in Prof. Caffo's lecture confirms that all three regressors are significant.
Appendix - Exploratory Analysis and Visualizations
Correlations


```{r}
mtcars_vars <- mtcars[, c(1, 6, 7, 9)]
mar.orig <- par()$mar  # save the original values 
par(mar = c(1, 1, 1, 1))  # set your new values 
pairs(mtcars_vars, panel = panel.smooth, col = 9 + mtcars$wt)
```

```{r}
par(mar = mar.orig)  # put the original values back 
cor(mtcars_vars)
```

Histograms
Nothing remarkable here except perhaps in the weight / wt histogram. The Cadillac Fleetwood, Lincoln Continental and Chrysler Imperial are quite a bit heavier than other cars in the dataset.


```{r}
library(ggplot2)
library(gridExtra)
mpg_dist <- qplot(mtcars_vars$mpg, fill = I("red"))
wt_dist <- qplot(mtcars_vars$wt, fill = I("lightblue"))
qsec_dist <- qplot(mtcars_vars$qsec, fill = I("purple"))
am_dist <- qplot(mtcars_vars$am, fill = I("green"))
grid.arrange(mpg_dist, wt_dist, qsec_dist, am_dist, ncol = 2)
```

Homogeneity of Variance Assumption
Box plots, comparison of the standard deviations of MPG by transmission type, and Levene's test indicate that the assumption of homogeneity of variance is questionable.
Side-by-side box plots

```{r}
mtcars_vars <- mtcars[, c(1, 6, 7, 9)]
mar.orig <- par()$mar  # save the original values 
par(mar = c(2, 2, 2, 2))  # set your new values 
boxplot(mtcars_vars[mtcars_vars$am == 1, ]$mpg, mtcars_vars[mtcars_vars$am == 
    0, ]$mpg, names = c("Manual", "Automatic"))
```


```{r}
par(mar = mar.orig)  # put the original values back
```

Standard Deviation of MPG by Transmission Type

```{r}
by(mtcars_vars$mpg, mtcars_vars$am, sd)
```
Levene's Test for Homogeneity of Variance

```{r}
library(car)
leveneTest(mpg ~ factor(am), data = mtcars_vars)
```
## Levene's Test for Homogeneity of Variance (center = median)

Residual Plot
There is a bit of a curve to the residual plot, so that it departs slightly from normality. The residuals for the Chrysler Imperial, Fiat 128, and Toyota Corolla are called out because they exert some influence on the shape of the curve.

```{r}
mar.orig <- par()$mar  # save the original values 
par(mar = c(2, 2, 2, 2))  # set your new values 
plot(bestfit, which = c(1:1))
```


```{r}
par(mar = mar.orig)  # put the original values back
```

